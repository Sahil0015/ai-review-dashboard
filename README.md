# ğŸ½ï¸ AI-Powered Yelp Review Analysis System

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.109-green.svg)](https://fastapi.tiangolo.com)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.29-red.svg)](https://streamlit.io)
[![MongoDB](https://img.shields.io/badge/MongoDB-Atlas-brightgreen.svg)](https://mongodb.com)
[![Groq](https://img.shields.io/badge/LLM-Groq%20LLaMA-purple.svg)](https://groq.com)

An intelligent review analysis system that uses LLMs to predict star ratings, generate summaries, and provide actionable business recommendations from Yelp reviews.

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Project Structure](#-project-structure)
- [Features](#-features)
- [Tech Stack](#-tech-stack)
- [Task 1: Prompt Engineering & Evaluation](#-task-1-prompt-engineering--evaluation)
- [Task 2: Streamlit AI Dashboard](#-task-2-full-stack-ai-dashboard)
- [Installation](#-installation)
- [Configuration](#-configuration)
- [Running the Application](#-running-the-application)
- [API Documentation](#-api-documentation)
- [Deployment](#-deployment)
- [Author](#-author)

---

## ğŸ¯ Overview

This project was developed as part of the **AI Intern Assessment**. It demonstrates:

1. **Prompt Engineering**: Designing, testing, and evaluating multiple prompt strategies for LLM-based review classification
2. **Full-Stack Development**: Building a production-ready AI-powered web application with real-time analytics

---

## ğŸ“ Project Structure

```
Ai-intern-assessment/
â”œâ”€â”€ Task1/                          # Prompt Engineering & Analysis
â”‚   â”œâ”€â”€ task1.ipynb                 # Jupyter notebook with full analysis
â”‚   â”œâ”€â”€ yelp.csv                    # Dataset (10,000 Yelp reviews)
â”‚   â”œâ”€â”€ design.txt                  # Design workflow
â”‚   â””â”€â”€ prompt_evaluation_metrics.csv
â”‚
â”œâ”€â”€ Task2/                          # Full-Stack Application
â”‚   â”œâ”€â”€ server/                     # FastAPI Backend
â”‚   â”‚   â”œâ”€â”€ app.py                  # Main API application
â”‚   â”‚   â”œâ”€â”€ config.py               # Groq/LLM configuration
â”‚   â”‚   â”œâ”€â”€ database.py             # MongoDB operations
â”‚   â”‚   â”œâ”€â”€ llm_service.py          # LLM prompts & calls
â”‚   â”‚   â””â”€â”€ models.py               # Pydantic models
â”‚   â”‚
â”‚   â”œâ”€â”€ client/                     # Streamlit Frontend
â”‚   â”‚   â”œâ”€â”€ user_app.py             # User review submission
â”‚   â”‚   â””â”€â”€ admin_app.py            # Admin analytics dashboard
â”‚   â”‚
â”‚   â””â”€â”€ data/                       # Local data backup
â”‚
â”œâ”€â”€ requirements.txt                # Python dependencies
â””â”€â”€ README.md                       # This file
```

---

## âœ¨ Features

### ğŸ”¬ Task 1: Prompt Engineering
- **3 Prompt Versions** with different strategies:
  - **V1 (Simple)**: Baseline single-shot classification
  - **V2 (Criteria-Based)**: Detailed rating criteria with examples
  - **V3 (Chain-of-Thought)**: Step-by-step reasoning approach
- **Comprehensive Evaluation**: Accuracy, MAE, RMSE, Validity Rate
- **Reliability Testing**: Consistency across multiple runs
- **Detailed Visualizations**: Confusion matrices, error distributions

### ğŸ–¥ï¸ Task 2: AI Dashboard

#### User Features
- â­ Submit reviews with 1-5 star ratings
- ğŸ¤– Get AI-predicted ratings with explanations
- ğŸ“ Receive AI-generated review summaries
- ğŸ’¡ View business recommendations
- ğŸ­ Sentiment analysis (Positive/Negative/Mixed)

#### Admin Features
- ğŸ“Š Real-time analytics dashboard
- ğŸ”„ Auto-refresh with configurable intervals
- ğŸ“ˆ Interactive charts (Rating & Sentiment distribution)
- ğŸ“‹ View all submissions with details
- ğŸ“¥ Export data to CSV
- ğŸ†• New submission indicators

---

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|------------|
| **LLM** | Groq API (LLaMA 3.1 8B Instant) |
| **Backend** | FastAPI + Uvicorn |
| **Frontend** | Streamlit |
| **Database** | MongoDB Atlas |
| **Data Validation** | Pydantic |
| **Visualization** | Plotly, Matplotlib, Seaborn |
| **ML/Data** | Pandas, NumPy, Scikit-learn |
| **Deployment** | Render (Backend), Streamlit Cloud (Frontend) |

---

## ğŸ”¬ Task 1: Prompt Engineering & Evaluation

### Workflow

```
Load Dataset (10,000 reviews)
        â†“
Sample 200 reviews (40 per star rating)
        â†“
Define 3 Prompt Versions
        â†“
Validate Input/Output with Pydantic
        â†“
Run LLM Predictions
        â†“
Calculate Metrics & Compare
        â†“
Reliability/Consistency Testing
        â†“
Final Analysis & Recommendations
```

### Prompt Strategies

| Version | Strategy | Description |
|---------|----------|-------------|
| **V1** | Simple | Direct classification prompt |
| **V2** | Criteria-Based | Explicit rating criteria (1-5â˜…) with examples |
| **V3** | Chain-of-Thought | Systematic analysis of positives, negatives, tone |

### Evaluation Metrics

- **Accuracy**: Exact match percentage
- **MAE**: Mean Absolute Error
- **RMSE**: Root Mean Square Error
- **Validity Rate**: % of valid JSON responses
- **Off-by-1/2+ Analysis**: Error distribution

---

## ğŸ–¥ï¸ Task 2: Full-Stack AI Dashboard

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User App      â”‚â”€â”€â”€â”€â–¶â”‚   FastAPI       â”‚â”€â”€â”€â”€â–¶â”‚   MongoDB       â”‚
â”‚  (Streamlit)    â”‚     â”‚   Backend       â”‚     â”‚   Atlas         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚   Admin App     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  (Streamlit)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Groq LLM      â”‚
â”‚   (LLaMA 3.1)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/` | Health check |
| `GET` | `/health` | Detailed health status |
| `POST` | `/api/user/submit` | Submit review for AI analysis |
| `POST` | `/api/predict/v1` | Predict with V1 prompt |
| `POST` | `/api/predict/v2` | Predict with V2 prompt |
| `POST` | `/api/predict/v3` | Predict with V3 prompt |
| `GET` | `/api/admin/submissions` | Get all submissions |
| `GET` | `/api/admin/analytics` | Get dashboard analytics |
| `GET` | `/api/admin/evaluations` | Get evaluation metrics |

---

## ğŸš€ Installation

### Prerequisites

- Python 3.10+
- MongoDB Atlas account
- Groq API key

### Setup

1. **Clone the repository**
   ```bash
   git clone https://github.com/Sahil0015/ai-review-dashboard.git
   cd ai-review-dashboard
   ```

2. **Create virtual environment**
   ```bash
   python -m venv myenv
   
   # Windows
   myenv\Scripts\activate
   
   # Linux/Mac
   source myenv/bin/activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

---

## âš™ï¸ Configuration

Create a `.env` file in the root directory:

```env
# Groq API
GROQ_API_KEY=your_groq_api_key_here

# MongoDB Atlas
MONGODB_URI=mongodb+srv://username:password@cluster.mongodb.net/?retryWrites=true&w=majority
```

### MongoDB Atlas Setup

1. Create a free cluster at [MongoDB Atlas](https://www.mongodb.com/atlas)
2. Create a database user with read/write permissions
3. Whitelist your IP address (or `0.0.0.0/0` for development)
4. Get your connection string and add it to `.env`

---

## â–¶ï¸ Running the Application

### Task 1: Jupyter Notebook
```bash
cd Task1
jupyter notebook task1.ipynb
```

### Task 2: Full Application

**1. Start the FastAPI Backend**
```bash
cd Task2/server
uvicorn app:app --host 0.0.0.0 --port 8000 --reload
```

**2. Start User Dashboard (new terminal)**
```bash
cd Task2/client
streamlit run user_app.py --server.port 8501
```

**3. Start Admin Dashboard (new terminal)**
```bash
cd Task2/client
streamlit run admin_app.py --server.port 8502
```

### Access Points

| Service | URL |
|---------|-----|
| Backend API | http://localhost:8000 |
| API Docs | http://localhost:8000/docs |
| User Dashboard | http://localhost:8501 |
| Admin Dashboard | http://localhost:8502 |

---

## ğŸ“š API Documentation

Once the server is running, access the interactive API documentation:

- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

### Example: Submit Review

```bash
curl -X POST "http://localhost:8000/api/user/submit" \
  -H "Content-Type: application/json" \
  -d '{
    "rating": 4,
    "review_text": "Great food and excellent service! The pasta was amazing, though the wait was a bit long."
  }'
```

**Response:**
```json
{
  "predicted_stars": 4,
  "explanation": "Mostly positive with minor complaint about wait time",
  "ai_summary": "Customer enjoyed the food quality and service...",
  "recommended_actions": ["Optimize wait times during peak hours", "..."],
  "sentiment": "Positive",
  "submission_id": "abc12345",
  "timestamp": "2025-12-07T10:30:00"
}
```

---

## ğŸŒ Deployment

### Backend (Render)

1. Push code to GitHub
2. Create a new **Web Service** on [Render](https://render.com)
3. Connect your GitHub repository
4. Configure:
   - **Build Command**: `pip install -r requirements.txt`
   - **Start Command**: `cd Task2/server && uvicorn app:app --host 0.0.0.0 --port $PORT`
5. Add environment variables (`GROQ_API_KEY`, `MONGODB_URI`)

### Frontend (Streamlit Cloud)

1. Deploy from GitHub at [Streamlit Cloud](https://streamlit.io/cloud)
2. Point to `Task2/client/user_app.py` or `admin_app.py`
3. Add secrets in Streamlit Cloud settings

### Live Demo

- **Backend API**: https://ai-review-dashboard.onrender.com
- **User App**: [Streamlit Cloud URL]
- **Admin App**: [Streamlit Cloud URL]

---

## ğŸ“Š Sample Results

### Prompt Comparison

| Metric | V1 (Simple) | V2 (Criteria) | V3 (CoT) |
|--------|-------------|---------------|----------|
| Accuracy | ~45% | ~52% | ~55% |
| MAE | ~0.85 | ~0.72 | ~0.68 |
| Validity | 98% | 99% | 99% |

*Results may vary based on sample selection and model behavior*

---

## ğŸ§ª Testing

```bash
# Run API health check
curl http://localhost:8000/health

# Test prediction endpoint
curl -X POST "http://localhost:8000/api/predict/v3" \
  -H "Content-Type: application/json" \
  -d '{"review_text": "Absolutely loved this place! Best pizza in town."}'
```

---

## ğŸ“ Logging

The application includes comprehensive logging:

- **Backend**: Structured logs with timestamps, levels, and function names
- **Database**: Connection status, operation success/failure logs
- **LLM Service**: API calls, retries, and error handling

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ‘¤ Author

**Sahil Aggarwal**

- ğŸ“§ Email: sahilaggarwal1532003@gmail.com
- ğŸ’¼ LinkedIn: [sahil-codes](https://www.linkedin.com/in/sahil-codes/)
- ğŸ™ GitHub: [Sahil0015](https://github.com/Sahil0015)

---

## ğŸ“„ License

This project is licensed under the MIT License â€” feel free to use, modify, and share with attribution.

---

## ğŸ™ Acknowledgments

- [Fynd](https://www.fynd.com/) for the assessment opportunity
- [Groq](https://groq.com/) for the fast LLM inference API
- [MongoDB](https://mongodb.com/) for the database platform
- [Streamlit](https://streamlit.io/) for the easy-to-use frontend framework

---

<p align="center">
  Made with â¤ï¸
</p>
